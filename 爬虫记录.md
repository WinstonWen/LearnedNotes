# 记录

## Scrapy-redis

### item
	使用前先初始化所有变量，否则存入mongodb时将会出现字段丢失

### spider
	pase中返回request,response,item必须使用yield，return只用于结束函数
	scrapy中首次请求函数微start_requests(self):请求源自self.start_urls
	scrapy-redis中首次请求函数make_request_from_data(self, data):请求源自redis数据库
### 下载中间件
	process_request()函数必须返回：None，Response对象，Request对象或raise IgnoreRequest
	1. 返回None,Scrapy将继续处理该request，执行其他的中间件中相应的方法
	2. 返回的是Response对象，scrapy将不会调用任何其他的process_request()或process_exception()方法
	3. 返回Request对象，scrapy则停止调用process_request()方法并重新调度返回的request。
	4. raise一个IgnoreRequest异常，则安装的下载中间件的process_exception()方法会被调用。
	参数:
	request(Request对象)--处理的request
	spider(Spider对象)--该request对应的spider
	
	process_response(self, request, spider):当下载器完成http请求，传递响应给引擎的时候调用
	1. 返回Response,该response会被在链中其他中间件的process_response()方法处理
	2. 返回Request对象，则中间件链停止，返回的request会被重新调度下载
	3. 抛出一个IgnorRequest异常，则调用Request对象的errback
	参数：
	request(Request对象)--response所对应的request
	response(Response对象)--被处理的response对象
	spider(Spider对象)--response所对应的spider
### setting
	BOT_NAME
	默认：“scrapybot”，使用startproject命令创建项目时，其被自动赋值
	CONCURRENT_ITEMS
	默认为100，Item Process(即Item Pipeline)同时处理（每个response的）item时最大值
	CONCURRENT_REQUEST
	默认为16，scrapy downloader并发请求的最大值
	LOG_ENABLED
	默认为True,是否启用logging
	DEFAULT_REQUEST_HEADERS
	默认如下：{'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8', 				'Accept-Language': 'en',}    scrapy http request使用的默认header
	LOG_ENCODING
	默认utt-8，logging中使用的编码
	LOG_LEVEL
	默认“DEBUG”，log中最低级别，可选级别有：CRITICAL,ERROR,WARNING,DEBUG
	USER_AGENT
	默认：“Scrapy/VERSION(....)”，爬取的默认User-Agent,除非被覆盖
	COOKIES_ENABLED=False，禁用cookies
### json
	{'value':null} 通过json.loads(),后得到的value值为空类型，如果进行连续get(),将出错
	切忌少用dict.get().get()或dict[][]
### 报错合集：
	1.启动时报错生成器没有meta属性：
			make_request_from_data函数必须使用return，不能用yield




